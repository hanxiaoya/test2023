{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:55:39.193317Z",
     "iopub.status.busy": "2023-08-08T00:55:39.192900Z",
     "iopub.status.idle": "2023-08-08T00:55:41.541164Z",
     "shell.execute_reply": "2023-08-08T00:55:41.539816Z",
     "shell.execute_reply.started": "2023-08-08T00:55:39.193281Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/THUDM/ChatGLM-6B.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:55:41.545486Z",
     "iopub.status.busy": "2023-08-08T00:55:41.544245Z",
     "iopub.status.idle": "2023-08-08T00:56:15.779182Z",
     "shell.execute_reply": "2023-08-08T00:56:15.777949Z",
     "shell.execute_reply.started": "2023-08-08T00:55:41.545443Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r ChatGLM-6B/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:56:15.781163Z",
     "iopub.status.busy": "2023-08-08T00:56:15.780765Z",
     "iopub.status.idle": "2023-08-08T00:56:27.572230Z",
     "shell.execute_reply": "2023-08-08T00:56:27.570971Z",
     "shell.execute_reply.started": "2023-08-08T00:56:15.781124Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install rouge_chinese nltk jieba datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:56:27.576698Z",
     "iopub.status.busy": "2023-08-08T00:56:27.576367Z",
     "iopub.status.idle": "2023-08-08T00:57:20.403956Z",
     "shell.execute_reply": "2023-08-08T00:57:20.402415Z",
     "shell.execute_reply.started": "2023-08-08T00:56:27.576669Z"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/THUDM/chatglm-6b-int4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:57:20.411497Z",
     "iopub.status.busy": "2023-08-08T00:57:20.411011Z",
     "iopub.status.idle": "2023-08-08T00:58:01.131433Z",
     "shell.execute_reply": "2023-08-08T00:58:01.130388Z",
     "shell.execute_reply.started": "2023-08-08T00:57:20.411450Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_path = \"chatglm-6b-int4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n",
    "# model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T00:58:01.133381Z",
     "iopub.status.busy": "2023-08-08T00:58:01.133003Z",
     "iopub.status.idle": "2023-08-08T00:58:57.579516Z",
     "shell.execute_reply": "2023-08-08T00:58:57.578456Z",
     "shell.execute_reply.started": "2023-08-08T00:58:01.133346Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# 准备提示语\n",
    "prompt = \"如何制作宫保鸡丁\"\n",
    "\n",
    "# 使用 IPython.display 流式打印模型输出\n",
    "for response, history in model.stream_chat(\n",
    "        tokenizer, prompt, history=[]):\n",
    "    clear_output(wait=True)\n",
    "    display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:58:57.582052Z",
     "iopub.status.busy": "2023-08-08T00:58:57.581356Z",
     "iopub.status.idle": "2023-08-08T00:59:02.738121Z",
     "shell.execute_reply": "2023-08-08T00:59:02.736866Z",
     "shell.execute_reply.started": "2023-08-08T00:58:57.582015Z"
    }
   },
   "outputs": [],
   "source": [
    "# 下载 ADGEN 数据集\n",
    "!wget -O AdvertiseGen.tar.gz https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:59:02.740439Z",
     "iopub.status.busy": "2023-08-08T00:59:02.740103Z",
     "iopub.status.idle": "2023-08-08T00:59:04.281967Z",
     "shell.execute_reply": "2023-08-08T00:59:04.280596Z",
     "shell.execute_reply.started": "2023-08-08T00:59:02.740412Z"
    }
   },
   "outputs": [],
   "source": [
    "# 解压数据集\n",
    "!tar -xzvf AdvertiseGen.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T00:59:04.286428Z",
     "iopub.status.busy": "2023-08-08T00:59:04.286038Z",
     "iopub.status.idle": "2023-08-08T00:59:04.292140Z",
     "shell.execute_reply": "2023-08-08T00:59:04.291012Z",
     "shell.execute_reply.started": "2023-08-08T00:59:04.286398Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-08T00:59:04.294523Z",
     "iopub.status.busy": "2023-08-08T00:59:04.293508Z",
     "iopub.status.idle": "2023-08-08T01:12:54.831815Z",
     "shell.execute_reply": "2023-08-08T01:12:54.830484Z",
     "shell.execute_reply.started": "2023-08-08T00:59:04.294488Z"
    }
   },
   "outputs": [],
   "source": [
    "# P-tuning v2\n",
    "!PRE_SEQ_LEN=128 && LR=2e-2 && CUDA_VISIBLE_DEVICES=0 python3 ChatGLM-6B/ptuning/main.py \\\n",
    "    --do_train \\\n",
    "    --train_file AdvertiseGen/train.json \\\n",
    "    --validation_file AdvertiseGen/dev.json \\\n",
    "    --prompt_column content \\\n",
    "    --response_column summary \\\n",
    "    --overwrite_cache \\\n",
    "    --model_name_or_path chatglm-6b-int4 \\\n",
    "    --output_dir output/adgen-chatglm-6b-int4-pt-$PRE_SEQ_LEN-$LR \\\n",
    "    --overwrite_output_dir \\\n",
    "    --max_source_length 64 \\\n",
    "    --max_target_length 64 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --predict_with_generate \\\n",
    "    --max_steps 100 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate $LR \\\n",
    "    --pre_seq_len $PRE_SEQ_LEN \\\n",
    "    --quantization_bit 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !PRE_SEQ_LEN=128 && CHECKPOINT_PATH=adgen-chatglm-6b-int4-pt-128-2e-2 && STEP=100 && CUDA_VISIBLE_DEVICES=0 python3 ChatGLM-6B/ptuning/main.py \\\n",
    "#     --do_predict \\\n",
    "#     --validation_file AdvertiseGen/dev.json \\\n",
    "#     --test_file AdvertiseGen/dev.json \\\n",
    "#     --overwrite_cache \\\n",
    "#     --prompt_column content \\\n",
    "#     --response_column summary \\\n",
    "#     --model_name_or_path chatglm-6b-int4 \\\n",
    "#     --ptuning_checkpoint ./output/$CHECKPOINT_PATH/checkpoint-$STEP \\\n",
    "#     --output_dir ./output/$CHECKPOINT_PATH \\\n",
    "#     --overwrite_output_dir \\\n",
    "#     --max_source_length 64 \\\n",
    "#     --max_target_length 64 \\\n",
    "#     --per_device_eval_batch_size 2 \\\n",
    "#     --predict_with_generate \\\n",
    "#     --pre_seq_len $PRE_SEQ_LEN \\\n",
    "#     --quantization_bit 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:12:54.840843Z",
     "iopub.status.busy": "2023-08-08T01:12:54.840407Z",
     "iopub.status.idle": "2023-08-08T01:13:21.811917Z",
     "shell.execute_reply": "2023-08-08T01:13:21.810585Z",
     "shell.execute_reply.started": "2023-08-08T01:12:54.840804Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "# Fine-tuning 后的表现测试，载入Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)\n",
    "model = AutoModel.from_pretrained(model_path, config=config, trust_remote_code=True)\n",
    "# 此处使用你的 ptuning 工作目录\n",
    "prefix_state_dict = torch.load(os.path.join(\"output/adgen-chatglm-6b-int4-pt-128-2e-2/checkpoint-100\", \"pytorch_model.bin\"))\n",
    "new_prefix_state_dict = {}\n",
    "for k, v in prefix_state_dict.items():\n",
    "    if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "        new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "# 根据需求可以进行量化，也可以直接使用：\n",
    "# model = model.quantize(4)\n",
    "model = model.half().cuda()\n",
    "model.transformer.prefix_encoder.float()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-08T01:13:21.820597Z",
     "iopub.status.busy": "2023-08-08T01:13:21.817667Z",
     "iopub.status.idle": "2023-08-08T01:13:34.908171Z",
     "shell.execute_reply": "2023-08-08T01:13:34.907062Z",
     "shell.execute_reply.started": "2023-08-08T01:13:21.820553Z"
    }
   },
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"类型#上衣\\*材质#牛仔布\\*颜色#白色\\*风格#简约\\*图案#刺绣\\*衣样式#外套\\*衣款式#破洞\", history=[])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
