{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-08T14:56:50.822830Z","iopub.execute_input":"2023-08-08T14:56:50.823480Z","iopub.status.idle":"2023-08-08T14:56:50.829803Z","shell.execute_reply.started":"2023-08-08T14:56:50.823443Z","shell.execute_reply":"2023-08-08T14:56:50.828893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 使用DeepSpeed进行全参数微调","metadata":{}},{"cell_type":"markdown","source":"## 安装依赖","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/THUDM/ChatGLM2-6B.git","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:56:50.874753Z","iopub.execute_input":"2023-08-08T14:56:50.875333Z","iopub.status.idle":"2023-08-08T14:56:53.138833Z","shell.execute_reply.started":"2023-08-08T14:56:50.875300Z","shell.execute_reply":"2023-08-08T14:56:53.137539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r ChatGLM2-6B/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:56:53.141236Z","iopub.execute_input":"2023-08-08T14:56:53.141603Z","iopub.status.idle":"2023-08-08T14:57:17.180974Z","shell.execute_reply.started":"2023-08-08T14:56:53.141573Z","shell.execute_reply":"2023-08-08T14:57:17.179786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_chinese nltk jieba datasets ","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:57:17.183107Z","iopub.execute_input":"2023-08-08T14:57:17.183556Z","iopub.status.idle":"2023-08-08T14:57:29.334965Z","shell.execute_reply.started":"2023-08-08T14:57:17.183496Z","shell.execute_reply":"2023-08-08T14:57:29.333726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 加载模型","metadata":{}},{"cell_type":"code","source":"!git clone https://huggingface.co/THUDM/chatglm2-6b-int4","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:57:29.338278Z","iopub.execute_input":"2023-08-08T14:57:29.338752Z","iopub.status.idle":"2023-08-08T14:58:21.071287Z","shell.execute_reply.started":"2023-08-08T14:57:29.338714Z","shell.execute_reply":"2023-08-08T14:58:21.070046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 加载模型\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_path = \"chatglm2-6b-int4\"\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().cuda()\n# model = model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:58:21.073206Z","iopub.execute_input":"2023-08-08T14:58:21.074585Z","iopub.status.idle":"2023-08-08T14:58:52.504022Z","shell.execute_reply.started":"2023-08-08T14:58:21.074543Z","shell.execute_reply":"2023-08-08T14:58:52.502901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, Markdown, clear_output\n\n# 准备提示语\nprompt = \"大模型需要具备的技能有哪些\"\n\n# 使用 IPython.display 流式打印模型输出\nfor response, history in model.stream_chat(\n        tokenizer, prompt, history=[]):\n    clear_output(wait=True)\n    display(Markdown(response))","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:58:52.505425Z","iopub.execute_input":"2023-08-08T14:58:52.505809Z","iopub.status.idle":"2023-08-08T14:59:13.040293Z","shell.execute_reply.started":"2023-08-08T14:58:52.505772Z","shell.execute_reply":"2023-08-08T14:59:13.038794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 模型微调","metadata":{}},{"cell_type":"code","source":"# 下载 ADGEN 数据集\n!wget -O AdvertiseGen.tar.gz https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:59:13.041975Z","iopub.execute_input":"2023-08-08T14:59:13.043159Z","iopub.status.idle":"2023-08-08T14:59:17.400757Z","shell.execute_reply.started":"2023-08-08T14:59:13.043115Z","shell.execute_reply":"2023-08-08T14:59:17.399412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 解压数据集\n!tar -xzvf AdvertiseGen.tar.gz","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:59:17.403178Z","iopub.execute_input":"2023-08-08T14:59:17.403499Z","iopub.status.idle":"2023-08-08T14:59:18.961977Z","shell.execute_reply.started":"2023-08-08T14:59:17.403467Z","shell.execute_reply":"2023-08-08T14:59:18.960613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install deepspeed","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:59:18.965418Z","iopub.execute_input":"2023-08-08T14:59:18.965873Z","iopub.status.idle":"2023-08-08T14:59:43.263114Z","shell.execute_reply.started":"2023-08-08T14:59:18.965825Z","shell.execute_reply":"2023-08-08T14:59:43.261777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:59:43.268261Z","iopub.execute_input":"2023-08-08T14:59:43.268614Z","iopub.status.idle":"2023-08-08T14:59:43.276552Z","shell.execute_reply.started":"2023-08-08T14:59:43.268583Z","shell.execute_reply":"2023-08-08T14:59:43.274586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !MASTER_PORT=$(shuf -n 1 -i 10000-65535)\n# !LR=1e-4 && CUDA_VISIBLE_DEVICES=0,1 deepspeed --num_gpus=2 --master_port $MASTER_PORT  ChatGLM2-6B/ptuning/main.py \\\n\n# LR=1e-4 \n!deepspeed --num_gpus=2 --master_port 520 ChatGLM2-6B/ptuning/main.py \\\n    --deepspeed ChatGLM2-6B/ptuning/deepspeed.json \\\n    --do_train \\\n    --train_file AdvertiseGen/train.json \\\n    --test_file AdvertiseGen/dev.json \\\n    --prompt_column content \\\n    --response_column summary \\\n    --overwrite_cache \\\n    --model_name_or_path chatglm2-6b-int4 \\\n    --output_dir ./output/adgen-chatglm2-6b-int4-pt \\\n    --overwrite_output_dir \\\n    --max_source_length 64 \\\n    --max_target_length 64 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 1 \\\n    --predict_with_generate \\\n    --max_steps 100 \\\n    --logging_steps 10 \\\n    --save_steps 100 \\\n    --learning_rate 2e-2 \\\n    --fp16","metadata":{"execution":{"iopub.status.busy":"2023-08-08T14:59:43.278387Z","iopub.execute_input":"2023-08-08T14:59:43.278772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 模型部署-单卡","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\n\n# Fine-tuning 后的表现测试，载入Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\nconfig = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)\nmodel = AutoModel.from_pretrained(model_path, config=config, trust_remote_code=True)\n# 此处使用你的 ptuning 工作目录\nprefix_state_dict = torch.load(os.path.join(\"output/adgen-chatglm2-6b-int4-pt/checkpoint-100\", \"pytorch_model.bin\"))\nnew_prefix_state_dict = {}\nfor k, v in prefix_state_dict.items():\n    if k.startswith(\"transformer.prefix_encoder.\"):\n        new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\nmodel.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n\n# 根据需求可以进行量化，也可以直接使用：\n# model = model.quantize(4)\nmodel = model.half().cuda()\nmodel.transformer.prefix_encoder.float()\nmodel = model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 模型部署（多卡部署）","metadata":{}},{"cell_type":"code","source":"# import torch\n# from transformers import AutoConfig, AutoModel, AutoTokenizer\n# from utils import load_model_on_gpus\n\n# # Fine-tuning 后的表现测试，载入Tokenizer\n# tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n# config = AutoConfig.from_pretrained(model_path, trust_remote_code=True, pre_seq_len=128)\n# model = load_model_on_gpus(model_path, num_gpus=2)\n\n# # 此处使用你的 ptuning 工作目录\n# prefix_state_dict = torch.load(os.path.join(\"output/adgen-chatglm2-6b-int4-pt/checkpoint-100\", \"pytorch_model.bin\"))\n# new_prefix_state_dict = {}\n# for k, v in prefix_state_dict.items():\n#     if k.startswith(\"transformer.prefix_encoder.\"):\n#         new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n# model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n\n# # 根据需求可以进行量化，也可以直接使用：\n# # model = model.quantize(4)\n# model = model.half().cuda()\n# model.transformer.prefix_encoder.float()\n# model = model.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# response, history = model.chat(tokenizer, \"类型#上衣\\*材质#牛仔布\\*颜色#白色\\*风格#简约\\*图案#刺绣\\*衣样式#外套\\*衣款式#破洞\", history=[])\n# response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}