{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc484e75",
   "metadata": {},
   "source": [
    "# GNN工具库\n",
    "当下GNN大火, 有两个库是最热门的: Deep Graph Library (DGL) 和 PyTorch Geometric (PyG). \n",
    "这两个库都很好用, 差别也不特别大 (DGL官网是有中文教程的)；但是PyG相对来说更基础一些, 教程与支持也更多一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebf1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import KarateClub\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a931625",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KarateClub()\n",
    "G = to_networkx(dataset[0], to_undirected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a039bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[34, 34], edge_index=[2, 156], y=[34], train_mask=[34])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b0ff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(dataset.num_features)\n",
    "print(dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d2824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.classifier = Linear(2, dataset.num_classes)\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        out = self.classifier(h)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d95d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(34, 4)\n",
      "  (conv2): GCNConv(4, 4)\n",
      "  (conv3): GCNConv(4, 2)\n",
      "  (classifier): Linear(in_features=2, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GCN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cdcea2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1800,  0.6862,  0.1598,  0.1413],\n",
       "        [-0.1946,  0.6587,  0.1327,  0.1069],\n",
       "        [-0.1860,  0.6638,  0.1398,  0.1144],\n",
       "        [-0.1890,  0.6638,  0.1388,  0.1139],\n",
       "        [-0.1999,  0.6750,  0.1442,  0.1249],\n",
       "        [-0.1905,  0.6825,  0.1533,  0.1351],\n",
       "        [-0.1895,  0.6814,  0.1528,  0.1341],\n",
       "        [-0.1931,  0.6610,  0.1351,  0.1099],\n",
       "        [-0.1940,  0.6523,  0.1278,  0.0997],\n",
       "        [-0.1952,  0.6484,  0.1243,  0.0950],\n",
       "        [-0.1975,  0.6764,  0.1460,  0.1269],\n",
       "        [-0.1836,  0.6835,  0.1564,  0.1375],\n",
       "        [-0.1935,  0.6638,  0.1372,  0.1130],\n",
       "        [-0.1913,  0.6574,  0.1328,  0.1060],\n",
       "        [-0.2014,  0.6454,  0.1198,  0.0904],\n",
       "        [-0.1991,  0.6445,  0.1198,  0.0898],\n",
       "        [-0.1890,  0.6801,  0.1518,  0.1326],\n",
       "        [-0.2028,  0.6605,  0.1315,  0.1076],\n",
       "        [-0.1974,  0.6422,  0.1186,  0.0874],\n",
       "        [-0.1994,  0.6531,  0.1267,  0.0997],\n",
       "        [-0.1955,  0.6516,  0.1268,  0.0986],\n",
       "        [-0.2052,  0.6534,  0.1250,  0.0990],\n",
       "        [-0.1981,  0.6461,  0.1215,  0.0918],\n",
       "        [-0.1786,  0.6709,  0.1479,  0.1238],\n",
       "        [-0.1739,  0.7036,  0.1758,  0.1624],\n",
       "        [-0.1733,  0.6988,  0.1722,  0.1570],\n",
       "        [-0.1913,  0.6391,  0.1181,  0.0849],\n",
       "        [-0.1778,  0.6798,  0.1554,  0.1343],\n",
       "        [-0.1900,  0.6650,  0.1394,  0.1151],\n",
       "        [-0.1862,  0.6448,  0.1244,  0.0924],\n",
       "        [-0.1988,  0.6479,  0.1226,  0.0937],\n",
       "        [-0.1781,  0.6868,  0.1609,  0.1423],\n",
       "        [-0.1845,  0.6473,  0.1270,  0.0955],\n",
       "        [-0.1801,  0.6489,  0.1297,  0.0982]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, h = model(dataset.x, dataset.edge_index)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08eec0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.432432770729065\n",
      "h:tensor([[ 0.0962,  0.0115],\n",
      "        [ 0.0235,  0.0129],\n",
      "        [ 0.0432,  0.0201],\n",
      "        [ 0.0401,  0.0162],\n",
      "        [ 0.0522, -0.0054],\n",
      "        [ 0.0775,  0.0011],\n",
      "        [ 0.0763,  0.0031],\n",
      "        [ 0.0300,  0.0131],\n",
      "        [ 0.0107,  0.0181],\n",
      "        [ 0.0014,  0.0195],\n",
      "        [ 0.0575, -0.0035],\n",
      "        [ 0.0867,  0.0090],\n",
      "        [ 0.0353,  0.0106],\n",
      "        [ 0.0242,  0.0179],\n",
      "        [-0.0114,  0.0138],\n",
      "        [-0.0109,  0.0173],\n",
      "        [ 0.0740,  0.0046],\n",
      "        [ 0.0188,  0.0012],\n",
      "        [-0.0139,  0.0210],\n",
      "        [ 0.0068,  0.0108],\n",
      "        [ 0.0077,  0.0168],\n",
      "        [ 0.0014,  0.0033],\n",
      "        [-0.0065,  0.0175],\n",
      "        [ 0.0657,  0.0242],\n",
      "        [ 0.1390,  0.0069],\n",
      "        [ 0.1296,  0.0110],\n",
      "        [-0.0140,  0.0309],\n",
      "        [ 0.0852,  0.0189],\n",
      "        [ 0.0415,  0.0141],\n",
      "        [ 0.0032,  0.0333],\n",
      "        [-0.0036,  0.0152],\n",
      "        [ 0.0995,  0.0135],\n",
      "        [ 0.0102,  0.0337],\n",
      "        [ 0.0181,  0.0381]], grad_fn=<TanhBackward0>)\n",
      "loss:1.4260591268539429\n",
      "h:tensor([[ 0.0184,  0.0236],\n",
      "        [-0.0443,  0.0277],\n",
      "        [-0.0268,  0.0378],\n",
      "        [-0.0150,  0.0300],\n",
      "        [ 0.0192,  0.0029],\n",
      "        [ 0.0416,  0.0088],\n",
      "        [ 0.0405,  0.0110],\n",
      "        [-0.0167,  0.0268],\n",
      "        [-0.0415,  0.0358],\n",
      "        [-0.0418,  0.0362],\n",
      "        [ 0.0246,  0.0048],\n",
      "        [ 0.0541,  0.0181],\n",
      "        [-0.0032,  0.0222],\n",
      "        [-0.0260,  0.0335],\n",
      "        [-0.0555,  0.0316],\n",
      "        [-0.0554,  0.0354],\n",
      "        [ 0.0444,  0.0120],\n",
      "        [-0.0204,  0.0129],\n",
      "        [-0.0581,  0.0394],\n",
      "        [-0.0367,  0.0255],\n",
      "        [-0.0364,  0.0342],\n",
      "        [-0.0381,  0.0156],\n",
      "        [-0.0511,  0.0354],\n",
      "        [ 0.0079,  0.0429],\n",
      "        [ 0.0913,  0.0203],\n",
      "        [ 0.0815,  0.0251],\n",
      "        [-0.0612,  0.0498],\n",
      "        [ 0.0334,  0.0353],\n",
      "        [-0.0038,  0.0301],\n",
      "        [-0.0528,  0.0538],\n",
      "        [-0.0544,  0.0329],\n",
      "        [ 0.0426,  0.0301],\n",
      "        [-0.0747,  0.0577],\n",
      "        [-0.0813,  0.0635]], grad_fn=<TanhBackward0>)\n",
      "loss:1.4194817543029785\n",
      "h:tensor([[-0.0634,  0.0322],\n",
      "        [-0.1143,  0.0408],\n",
      "        [-0.0987,  0.0546],\n",
      "        [-0.0721,  0.0425],\n",
      "        [-0.0155,  0.0091],\n",
      "        [ 0.0035,  0.0138],\n",
      "        [ 0.0024,  0.0162],\n",
      "        [-0.0649,  0.0397],\n",
      "        [-0.0948,  0.0535],\n",
      "        [-0.0859,  0.0529],\n",
      "        [-0.0102,  0.0110],\n",
      "        [ 0.0199,  0.0253],\n",
      "        [-0.0432,  0.0326],\n",
      "        [-0.0776,  0.0487],\n",
      "        [-0.1004,  0.0495],\n",
      "        [-0.1007,  0.0535],\n",
      "        [ 0.0128,  0.0169],\n",
      "        [-0.0607,  0.0236],\n",
      "        [-0.1029,  0.0579],\n",
      "        [-0.0812,  0.0399],\n",
      "        [-0.0814,  0.0517],\n",
      "        [-0.0789,  0.0270],\n",
      "        [-0.0964,  0.0534],\n",
      "        [-0.0501,  0.0616],\n",
      "        [ 0.0442,  0.0332],\n",
      "        [ 0.0340,  0.0387],\n",
      "        [-0.1092,  0.0687],\n",
      "        [-0.0186,  0.0517],\n",
      "        [-0.0496,  0.0462],\n",
      "        [-0.1095,  0.0743],\n",
      "        [-0.1063,  0.0505],\n",
      "        [-0.0144,  0.0465],\n",
      "        [-0.1611,  0.0808],\n",
      "        [-0.1822,  0.0875]], grad_fn=<TanhBackward0>)\n",
      "loss:1.4125844240188599\n",
      "h:tensor([[-0.1476,  0.0362],\n",
      "        [-0.1858,  0.0514],\n",
      "        [-0.1721,  0.0697],\n",
      "        [-0.1307,  0.0532],\n",
      "        [-0.0516,  0.0127],\n",
      "        [-0.0361,  0.0156],\n",
      "        [-0.0372,  0.0182],\n",
      "        [-0.1141,  0.0516],\n",
      "        [-0.1491,  0.0708],\n",
      "        [-0.1309,  0.0693],\n",
      "        [-0.0463,  0.0145],\n",
      "        [-0.0152,  0.0307],\n",
      "        [-0.0843,  0.0414],\n",
      "        [-0.1301,  0.0632],\n",
      "        [-0.1462,  0.0672],\n",
      "        [-0.1467,  0.0714],\n",
      "        [-0.0200,  0.0187],\n",
      "        [-0.1021,  0.0330],\n",
      "        [-0.1486,  0.0762],\n",
      "        [-0.1265,  0.0536],\n",
      "        [-0.1273,  0.0690],\n",
      "        [-0.1205,  0.0371],\n",
      "        [-0.1426,  0.0711],\n",
      "        [-0.1089,  0.0797],\n",
      "        [-0.0037,  0.0447],\n",
      "        [-0.0143,  0.0511],\n",
      "        [-0.1582,  0.0872],\n",
      "        [-0.0714,  0.0673],\n",
      "        [-0.0964,  0.0618],\n",
      "        [-0.1671,  0.0944],\n",
      "        [-0.1590,  0.0677],\n",
      "        [-0.0724,  0.0619],\n",
      "        [-0.2483,  0.1022],\n",
      "        [-0.2834,  0.1090]], grad_fn=<TanhBackward0>)\n",
      "loss:1.4053231477737427\n",
      "h:tensor([[-0.2302,  0.0407],\n",
      "        [-0.2560,  0.0631],\n",
      "        [-0.2445,  0.0865],\n",
      "        [-0.1888,  0.0649],\n",
      "        [-0.0875,  0.0158],\n",
      "        [-0.0752,  0.0167],\n",
      "        [-0.0764,  0.0195],\n",
      "        [-0.1630,  0.0645],\n",
      "        [-0.2029,  0.0896],\n",
      "        [-0.1758,  0.0869],\n",
      "        [-0.0822,  0.0176],\n",
      "        [-0.0497,  0.0360],\n",
      "        [-0.1252,  0.0506],\n",
      "        [-0.1822,  0.0789],\n",
      "        [-0.1919,  0.0863],\n",
      "        [-0.1926,  0.0906],\n",
      "        [-0.0522,  0.0197],\n",
      "        [-0.1433,  0.0430],\n",
      "        [-0.1942,  0.0958],\n",
      "        [-0.1716,  0.0683],\n",
      "        [-0.1731,  0.0876],\n",
      "        [-0.1620,  0.0477],\n",
      "        [-0.1886,  0.0901],\n",
      "        [-0.1672,  0.0994],\n",
      "        [-0.0513,  0.0572],\n",
      "        [-0.0622,  0.0645],\n",
      "        [-0.2070,  0.1069],\n",
      "        [-0.1237,  0.0844],\n",
      "        [-0.1431,  0.0787],\n",
      "        [-0.2241,  0.1162],\n",
      "        [-0.2113,  0.0864],\n",
      "        [-0.1300,  0.0788],\n",
      "        [-0.3330,  0.1258],\n",
      "        [-0.3802,  0.1329]], grad_fn=<TanhBackward0>)\n",
      "loss:1.3977205753326416\n",
      "h:tensor([[-0.3092,  0.0464],\n",
      "        [-0.3238,  0.0765],\n",
      "        [-0.3145,  0.1055],\n",
      "        [-0.2455,  0.0779],\n",
      "        [-0.1227,  0.0189],\n",
      "        [-0.1133,  0.0176],\n",
      "        [-0.1145,  0.0207],\n",
      "        [-0.2111,  0.0786],\n",
      "        [-0.2555,  0.1104],\n",
      "        [-0.2201,  0.1060],\n",
      "        [-0.1172,  0.0206],\n",
      "        [-0.0832,  0.0417],\n",
      "        [-0.1655,  0.0605],\n",
      "        [-0.2333,  0.0962],\n",
      "        [-0.2367,  0.1070],\n",
      "        [-0.2376,  0.1114],\n",
      "        [-0.0833,  0.0204],\n",
      "        [-0.1838,  0.0537],\n",
      "        [-0.2390,  0.1171],\n",
      "        [-0.2157,  0.0842],\n",
      "        [-0.2180,  0.1079],\n",
      "        [-0.2027,  0.0592],\n",
      "        [-0.2336,  0.1108],\n",
      "        [-0.2239,  0.1212],\n",
      "        [-0.0977,  0.0708],\n",
      "        [-0.1090,  0.0791],\n",
      "        [-0.2548,  0.1284],\n",
      "        [-0.1748,  0.1031],\n",
      "        [-0.1890,  0.0970],\n",
      "        [-0.2797,  0.1401],\n",
      "        [-0.2624,  0.1070],\n",
      "        [-0.1861,  0.0975],\n",
      "        [-0.4130,  0.1525],\n",
      "        [-0.4700,  0.1602]], grad_fn=<TanhBackward0>)\n",
      "loss:1.3898134231567383\n",
      "h:tensor([[-0.3820,  0.0533],\n",
      "        [-0.3871,  0.0913],\n",
      "        [-0.3801,  0.1266],\n",
      "        [-0.2993,  0.0920],\n",
      "        [-0.1557,  0.0218],\n",
      "        [-0.1488,  0.0183],\n",
      "        [-0.1500,  0.0215],\n",
      "        [-0.2572,  0.0938],\n",
      "        [-0.3056,  0.1329],\n",
      "        [-0.2626,  0.1264],\n",
      "        [-0.1502,  0.0234],\n",
      "        [-0.1145,  0.0475],\n",
      "        [-0.2039,  0.0710],\n",
      "        [-0.2819,  0.1149],\n",
      "        [-0.2796,  0.1293],\n",
      "        [-0.2806,  0.1338],\n",
      "        [-0.1121,  0.0205],\n",
      "        [-0.2224,  0.0651],\n",
      "        [-0.2818,  0.1399],\n",
      "        [-0.2579,  0.1013],\n",
      "        [-0.2610,  0.1297],\n",
      "        [-0.2416,  0.0713],\n",
      "        [-0.2767,  0.1331],\n",
      "        [-0.2776,  0.1448],\n",
      "        [-0.1416,  0.0854],\n",
      "        [-0.1532,  0.0948],\n",
      "        [-0.3006,  0.1514],\n",
      "        [-0.2234,  0.1234],\n",
      "        [-0.2330,  0.1168],\n",
      "        [-0.3324,  0.1660],\n",
      "        [-0.3112,  0.1292],\n",
      "        [-0.2393,  0.1178],\n",
      "        [-0.4862,  0.1822],\n",
      "        [-0.5500,  0.1908]], grad_fn=<TanhBackward0>)\n",
      "loss:1.3816187381744385\n",
      "h:tensor([[-0.4444,  0.0604],\n",
      "        [-0.4429,  0.1069],\n",
      "        [-0.4384,  0.1490],\n",
      "        [-0.3475,  0.1067],\n",
      "        [-0.1843,  0.0241],\n",
      "        [-0.1792,  0.0182],\n",
      "        [-0.1805,  0.0216],\n",
      "        [-0.2988,  0.1095],\n",
      "        [-0.3514,  0.1567],\n",
      "        [-0.3017,  0.1479],\n",
      "        [-0.1787,  0.0257],\n",
      "        [-0.1416,  0.0530],\n",
      "        [-0.2382,  0.0815],\n",
      "        [-0.3261,  0.1344],\n",
      "        [-0.3191,  0.1529],\n",
      "        [-0.3200,  0.1575],\n",
      "        [-0.1363,  0.0198],\n",
      "        [-0.2573,  0.0767],\n",
      "        [-0.3212,  0.1640],\n",
      "        [-0.2962,  0.1190],\n",
      "        [-0.3006,  0.1529],\n",
      "        [-0.2767,  0.0836],\n",
      "        [-0.3162,  0.1566],\n",
      "        [-0.3262,  0.1698],\n",
      "        [-0.1807,  0.1006],\n",
      "        [-0.1926,  0.1113],\n",
      "        [-0.3426,  0.1758],\n",
      "        [-0.2673,  0.1447],\n",
      "        [-0.2731,  0.1376],\n",
      "        [-0.3804,  0.1935],\n",
      "        [-0.3557,  0.1527],\n",
      "        [-0.2870,  0.1393],\n",
      "        [-0.5499,  0.2143],\n",
      "        [-0.6176,  0.2241]], grad_fn=<TanhBackward0>)\n",
      "loss:1.3731058835983276\n",
      "h:tensor([[-0.4941,  0.0664],\n",
      "        [-0.4891,  0.1223],\n",
      "        [-0.4874,  0.1720],\n",
      "        [-0.3879,  0.1212],\n",
      "        [-0.2062,  0.0252],\n",
      "        [-0.2020,  0.0166],\n",
      "        [-0.2034,  0.0203],\n",
      "        [-0.3342,  0.1252],\n",
      "        [-0.3910,  0.1812],\n",
      "        [-0.3357,  0.1700],\n",
      "        [-0.2004,  0.0267],\n",
      "        [-0.1629,  0.0579],\n",
      "        [-0.2667,  0.0916],\n",
      "        [-0.3638,  0.1541],\n",
      "        [-0.3536,  0.1774],\n",
      "        [-0.3546,  0.1821],\n",
      "        [-0.1538,  0.0177],\n",
      "        [-0.2863,  0.0880],\n",
      "        [-0.3558,  0.1890],\n",
      "        [-0.3290,  0.1369],\n",
      "        [-0.3353,  0.1769],\n",
      "        [-0.3061,  0.0956],\n",
      "        [-0.3508,  0.1811],\n",
      "        [-0.3678,  0.1957],\n",
      "        [-0.2128,  0.1160],\n",
      "        [-0.2253,  0.1279],\n",
      "        [-0.3794,  0.2011],\n",
      "        [-0.3047,  0.1666],\n",
      "        [-0.3077,  0.1589],\n",
      "        [-0.4221,  0.2221],\n",
      "        [-0.3943,  0.1770],\n",
      "        [-0.3272,  0.1611],\n",
      "        [-0.6029,  0.2479],\n",
      "        [-0.6720,  0.2588]], grad_fn=<TanhBackward0>)\n",
      "loss:1.3641955852508545\n",
      "h:tensor([[-0.5313,  0.0700],\n",
      "        [-0.5257,  0.1367],\n",
      "        [-0.5270,  0.1945],\n",
      "        [-0.4201,  0.1347],\n",
      "        [-0.2200,  0.0243],\n",
      "        [-0.2158,  0.0126],\n",
      "        [-0.2173,  0.0165],\n",
      "        [-0.3626,  0.1401],\n",
      "        [-0.4238,  0.2058],\n",
      "        [-0.3642,  0.1923],\n",
      "        [-0.2141,  0.0258],\n",
      "        [-0.1772,  0.0615],\n",
      "        [-0.2886,  0.1007],\n",
      "        [-0.3945,  0.1733],\n",
      "        [-0.3827,  0.2021],\n",
      "        [-0.3836,  0.2069],\n",
      "        [-0.1631,  0.0134],\n",
      "        [-0.3089,  0.0983],\n",
      "        [-0.3849,  0.2143],\n",
      "        [-0.3554,  0.1542],\n",
      "        [-0.3645,  0.2013],\n",
      "        [-0.3291,  0.1068],\n",
      "        [-0.3799,  0.2058],\n",
      "        [-0.4019,  0.2215],\n",
      "        [-0.2370,  0.1306],\n",
      "        [-0.2502,  0.1439],\n",
      "        [-0.4104,  0.2268],\n",
      "        [-0.3349,  0.1883],\n",
      "        [-0.3360,  0.1802],\n",
      "        [-0.4572,  0.2510],\n",
      "        [-0.4265,  0.2014],\n",
      "        [-0.3591,  0.1825],\n",
      "        [-0.6457,  0.2819],\n",
      "        [-0.7148,  0.2939]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(data):\n",
    "    optimizer.zero_grad()\n",
    "    out, h = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, h\n",
    "for epoch in range(10):\n",
    "    loss, h = train(dataset)\n",
    "    print(f'loss:{loss}')\n",
    "    print(f'h:{h}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bdc0d",
   "metadata": {},
   "source": [
    "# 任务一：节点分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d03316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\aconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# 加载cora数据集\n",
    "dataset = Planetoid(root='./dataset/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9fc0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "包含的类别数: 7\n",
      "边特征的维度: 0\n",
      "节点特征的维度： 1433\n",
      "边的数量: 5278.0\n",
      "节点的数量: 2708\n",
      "节点属性特征: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "边: tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])\n",
      "节点类别数量： [(0, 351), (1, 217), (2, 418), (3, 818), (4, 426), (5, 298), (6, 180)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\aconda\\envs\\pytorch\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(dataset.data)\n",
    "print('包含的类别数:', dataset.num_classes)\n",
    "print('边特征的维度:', dataset.num_edge_features)\n",
    "print('节点特征的维度：', dataset.num_node_features)\n",
    "\n",
    "print('边的数量:', dataset.data.edge_index.shape[1] / 2)\n",
    "print('节点的数量:', dataset.data.x.shape[0])\n",
    "print('节点属性特征:', dataset.data.x)\n",
    "print(\"边:\", dataset.data.edge_index)\n",
    "print(\"节点类别数量：\", sorted(Counter(dataset[0].y.tolist()).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239ad935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集节点数量: tensor(140)\n",
      "验证集节点数量: tensor(500)\n",
      "测试集节点数量: tensor(1000)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集、测试集、验证集数据量\n",
    "print('训练集节点数量:', sum(dataset.data.train_mask))\n",
    "print('验证集节点数量:', sum(dataset.data.val_mask))\n",
    "print('测试集节点数量:', sum(dataset.data.test_mask))\n",
    "# 检查数据集是否是无向图\n",
    "print(dataset.data.is_undirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2281ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 4,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6298cc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.x\n",
    "# dataset.data.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b4b0d",
   "metadata": {},
   "source": [
    "## 1.1 使用lightgbm进行节点分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ac3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bf6e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\aconda\\envs\\pytorch\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "X = dataset.data.x\n",
    "y = dataset.data.y\n",
    "le = LabelEncoder()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.8,stratify=y,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dadfadd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9430189269234441\n",
      "0.9404446743313917\n"
     ]
    }
   ],
   "source": [
    "clf = lgb.LGBMClassifier(max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovr'))\n",
    "print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce814b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 5, 4, 3, 1, 3, 5, 4, 3, 4, 2, 3, 2, 3, 1, 2, 1, 6, 6, 1, 2, 5, 2,\n",
       "        3, 5, 6, 0, 6, 5, 3, 2, 4, 5, 0, 4, 3, 2, 3, 4, 0, 3, 4, 3, 3, 0, 6, 2,\n",
       "        3, 0, 6, 3, 3, 4, 5, 0, 3, 3, 6, 2, 2, 4, 2, 2, 3, 2, 4, 6, 2, 3, 2, 3,\n",
       "        3, 0, 3, 1, 2, 5, 0, 2, 1, 3, 3, 4, 4, 3, 4, 3, 0, 5, 1, 3, 4, 2, 4, 5,\n",
       "        0, 2, 2, 2, 1, 5, 2, 0, 6, 5, 5, 1, 0, 0, 3, 2, 0, 2, 0, 0, 3, 1, 4, 3,\n",
       "        0, 2, 0, 3, 5, 3, 0, 5, 4, 2, 3, 3, 4, 2, 5, 2, 2, 3, 5, 0, 3, 2, 3, 0,\n",
       "        3, 3, 1, 6, 0, 3, 2, 3, 0, 4, 4, 0, 3, 3, 0, 5, 5, 4, 4, 1, 3, 3, 4, 3,\n",
       "        2, 4, 3, 3, 3, 6, 4, 3, 3, 4, 3, 0, 4, 5, 4, 0, 3, 3, 4, 3, 3, 3, 3, 3,\n",
       "        0, 3, 3, 1, 3, 5, 4, 5, 3, 2, 4, 6, 2, 2, 3, 2, 5, 0, 2, 2, 5, 5, 3, 4,\n",
       "        3, 0, 3, 3, 3, 0, 5, 0, 0, 2, 2, 2, 3, 6, 1, 1, 3, 3, 3, 0, 3, 2, 2, 3,\n",
       "        2, 3, 5, 5, 4, 2, 3, 5, 4, 1, 5, 2, 2, 2, 4, 2, 3, 4, 3, 0, 0, 5, 3, 0,\n",
       "        0, 3, 4, 2, 1, 2, 5, 3, 6, 2, 4, 3, 5, 4, 3, 3, 5, 4, 0, 3, 4, 3, 3, 5,\n",
       "        0, 3, 4, 1, 3, 2, 3, 2, 6, 3, 1, 3, 6, 3, 3, 6, 2, 6, 4, 3, 0, 1, 0, 1,\n",
       "        6, 4, 0, 3, 0, 2, 3, 4, 3, 0, 1, 5, 0, 3, 2, 2, 6, 4, 3, 4, 1, 3, 0, 2,\n",
       "        6, 3, 0, 3, 4, 3, 3, 1, 4, 1, 4, 2, 0, 3, 3, 3, 0, 1, 4, 1, 2, 6, 0, 5,\n",
       "        0, 3, 1, 2, 2, 5, 2, 6, 4, 4, 5, 4, 3, 1, 3, 0, 4, 4, 4, 5, 3, 5, 5, 4,\n",
       "        3, 3, 2, 4, 4, 3, 0, 0, 4, 0, 6, 3, 0, 5, 3, 1, 2, 3, 0, 3, 0, 4, 3, 3,\n",
       "        2, 4, 2, 0, 3, 0, 4, 5, 1, 1, 5, 3, 1, 6, 4, 4, 0, 4, 3, 4, 4, 0, 0, 5,\n",
       "        1, 3, 6, 0, 4, 6, 3, 3, 4, 3, 4, 1, 3, 3, 6, 6, 2, 5, 2, 5, 3, 4, 1, 3,\n",
       "        3, 3, 4, 3, 6, 4, 1, 0, 4, 6, 3, 1, 5, 2, 5, 1, 0, 5, 3, 3, 3, 2, 4, 3,\n",
       "        6, 4, 4, 5, 3, 6, 3, 4, 2, 5, 6, 4, 2, 3, 3, 4, 2, 3, 0, 3, 4, 3, 3, 3,\n",
       "        3, 0, 1, 2, 3, 6, 3, 3, 3, 2, 1, 5, 3, 5, 5, 5, 2, 6, 1, 3, 2, 2, 3, 3,\n",
       "        3, 2, 0, 5, 3, 4, 5, 3, 4, 5, 2, 3, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658e7a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.86107665e-01, 2.28544631e-02, 1.02919859e-01, ...,\n",
       "        1.44850256e-01, 7.57919229e-02, 5.82738784e-02],\n",
       "       [1.27110723e-02, 6.14518253e-02, 1.17196214e-01, ...,\n",
       "        7.49725675e-02, 6.07052655e-02, 1.77596490e-02],\n",
       "       [9.57913370e-02, 3.01656627e-02, 1.39774053e-01, ...,\n",
       "        1.13701825e-01, 1.42378653e-01, 5.72503241e-02],\n",
       "       ...,\n",
       "       [1.41410829e-02, 6.06019201e-03, 1.49348031e-02, ...,\n",
       "        2.03237378e-02, 4.90583700e-03, 6.65862439e-03],\n",
       "       [3.97481008e-01, 4.99240999e-02, 2.28238399e-01, ...,\n",
       "        2.85387279e-02, 5.98649840e-02, 6.65181161e-02],\n",
       "       [3.25999807e-03, 5.07200630e-04, 9.82180698e-01, ...,\n",
       "        3.71246518e-03, 1.75644501e-03, 1.20816185e-03]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict_proba(X_valid)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d4cf1e",
   "metadata": {},
   "source": [
    "## 1.2 引入传统的图算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adc33022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\aconda\\envs\\pytorch\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1423</th>\n",
       "      <th>1424</th>\n",
       "      <th>1425</th>\n",
       "      <th>1426</th>\n",
       "      <th>1427</th>\n",
       "      <th>1428</th>\n",
       "      <th>1429</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>2703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>2704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>2705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>2706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>2707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2708 rows × 1434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      node_id    0    1    2    3    4    5    6    7    8  ...  1423  1424  \\\n",
       "0           0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1           1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2           2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3           3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4           4  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "2703     2703  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2704     2704  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2705     2705  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2706     2706  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2707     2707  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "      1425  1426  1427  1428  1429  1430  1431  1432  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "2703   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2704   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2705   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2706   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2707   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[2708 rows x 1434 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(dataset.data.x, index=range(0,len(dataset.data.y))).reset_index()\n",
    "X = X.rename(columns={'index':'node_id'})\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d23832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp \n",
    "import networkx as nx\n",
    "from torch_geometric.utils.convert import to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45a8c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\aconda\\envs\\pytorch\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "G = to_networkx(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "268a0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['degree'] = X.node_id.map(nx.degree_centrality(G))  # 度中心度\n",
    "X['pagerank'] = X.node_id.map(nx.pagerank(G)) # PageRank 中心度\n",
    "X['betweenness'] = X.node_id.map(nx.betweenness_centrality(G)) # 介数中心度\n",
    "# nx.closeness_centrality(G) # 接近中心度\n",
    "X['harmonic'] = X.node_id.map(nx.harmonic_centrality(G)) # 计算节点的谐波中心度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7888c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1427</th>\n",
       "      <th>1428</th>\n",
       "      <th>1429</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "      <th>degree</th>\n",
       "      <th>pagerank</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>harmonic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>9.766154e-07</td>\n",
       "      <td>417.066364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>1.080477e-03</td>\n",
       "      <td>436.741533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>4.050816e-03</td>\n",
       "      <td>531.907612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>5.511762e-04</td>\n",
       "      <td>458.192627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>2703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>2704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>2705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>2706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>3.963528e-05</td>\n",
       "      <td>398.100924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>2707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>1.677598e-04</td>\n",
       "      <td>460.032262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2708 rows × 1438 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      node_id    0    1    2    3    4    5    6    7    8  ...  1427  1428  \\\n",
       "0           0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1           1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2           2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3           3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4           4  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "2703     2703  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2704     2704  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2705     2705  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2706     2706  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2707     2707  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "      1429  1430  1431  1432    degree  pagerank   betweenness    harmonic  \n",
       "0      0.0   0.0   0.0   0.0  0.002216  0.000336  9.766154e-07  417.066364  \n",
       "1      0.0   0.0   0.0   0.0  0.002216  0.000385  1.080477e-03  436.741533  \n",
       "2      0.0   0.0   0.0   0.0  0.003694  0.000515  4.050816e-03  531.907612  \n",
       "3      0.0   0.0   0.0   0.0  0.000739  0.000369  0.000000e+00    1.000000  \n",
       "4      0.0   0.0   0.0   0.0  0.003694  0.000396  5.511762e-04  458.192627  \n",
       "...    ...   ...   ...   ...       ...       ...           ...         ...  \n",
       "2703   0.0   0.0   0.0   0.0  0.000739  0.000369  0.000000e+00    1.000000  \n",
       "2704   0.0   0.0   0.0   0.0  0.000739  0.000369  0.000000e+00    1.000000  \n",
       "2705   0.0   0.0   0.0   0.0  0.000739  0.000369  0.000000e+00    1.000000  \n",
       "2706   0.0   0.0   0.0   0.0  0.002955  0.000380  3.963528e-05  398.100924  \n",
       "2707   0.0   0.0   0.0   0.0  0.002955  0.000355  1.677598e-04  460.032262  \n",
       "\n",
       "[2708 rows x 1438 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "009f5bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{0,\n",
       "  1,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  24,\n",
       "  25,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  107,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  185,\n",
       "  186,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  223,\n",
       "  224,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  248,\n",
       "  249,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  285,\n",
       "  286,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  475,\n",
       "  476,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  584,\n",
       "  586,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  810,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  844,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  872,\n",
       "  873,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  882,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  931,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  965,\n",
       "  966,\n",
       "  968,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  1000,\n",
       "  1002,\n",
       "  1003,\n",
       "  1004,\n",
       "  1006,\n",
       "  1007,\n",
       "  1008,\n",
       "  1009,\n",
       "  1010,\n",
       "  1011,\n",
       "  1012,\n",
       "  1013,\n",
       "  1014,\n",
       "  1015,\n",
       "  1016,\n",
       "  1017,\n",
       "  1018,\n",
       "  1019,\n",
       "  1020,\n",
       "  1021,\n",
       "  1022,\n",
       "  1023,\n",
       "  1025,\n",
       "  1026,\n",
       "  1027,\n",
       "  1028,\n",
       "  1029,\n",
       "  1030,\n",
       "  1031,\n",
       "  1033,\n",
       "  1035,\n",
       "  1037,\n",
       "  1038,\n",
       "  1039,\n",
       "  1040,\n",
       "  1041,\n",
       "  1042,\n",
       "  1043,\n",
       "  1045,\n",
       "  1046,\n",
       "  1047,\n",
       "  1049,\n",
       "  1050,\n",
       "  1051,\n",
       "  1052,\n",
       "  1053,\n",
       "  1054,\n",
       "  1055,\n",
       "  1057,\n",
       "  1058,\n",
       "  1060,\n",
       "  1061,\n",
       "  1062,\n",
       "  1063,\n",
       "  1064,\n",
       "  1065,\n",
       "  1066,\n",
       "  1067,\n",
       "  1068,\n",
       "  1069,\n",
       "  1070,\n",
       "  1071,\n",
       "  1072,\n",
       "  1073,\n",
       "  1074,\n",
       "  1075,\n",
       "  1076,\n",
       "  1077,\n",
       "  1078,\n",
       "  1079,\n",
       "  1080,\n",
       "  1081,\n",
       "  1082,\n",
       "  1083,\n",
       "  1084,\n",
       "  1085,\n",
       "  1086,\n",
       "  1087,\n",
       "  1088,\n",
       "  1089,\n",
       "  1090,\n",
       "  1091,\n",
       "  1092,\n",
       "  1093,\n",
       "  1094,\n",
       "  ...},\n",
       " {3, 2544},\n",
       " {7, 208},\n",
       " {12, 1001, 1318, 2661, 2662},\n",
       " {23,\n",
       "  92,\n",
       "  108,\n",
       "  144,\n",
       "  145,\n",
       "  213,\n",
       "  495,\n",
       "  537,\n",
       "  898,\n",
       "  1165,\n",
       "  1327,\n",
       "  1328,\n",
       "  1504,\n",
       "  1593,\n",
       "  1647,\n",
       "  1698,\n",
       "  1835,\n",
       "  1836,\n",
       "  2157,\n",
       "  2158,\n",
       "  2159,\n",
       "  2160,\n",
       "  2161,\n",
       "  2192,\n",
       "  2209,\n",
       "  2622},\n",
       " {26, 99, 122, 123, 127, 2454, 2455, 2604},\n",
       " {31, 1594},\n",
       " {44, 1582, 2624, 2701},\n",
       " {66, 2631},\n",
       " {75, 84, 284, 583, 2222, 2223, 2224, 2225, 2226},\n",
       " {106, 2461},\n",
       " {117, 259, 2537},\n",
       " {167, 168, 1056, 2437, 2438, 2482},\n",
       " {184, 520},\n",
       " {187, 1208},\n",
       " {200, 1439, 2676},\n",
       " {222, 821},\n",
       " {225, 2255},\n",
       " {247, 2583},\n",
       " {250, 2429},\n",
       " {287, 2705},\n",
       " {292, 1036, 2562},\n",
       " {307, 991},\n",
       " {369, 385, 2483, 2484},\n",
       " {380, 477, 930, 2569},\n",
       " {390, 1108},\n",
       " {404, 1170, 1476},\n",
       " {422, 545},\n",
       " {431, 2694, 2695},\n",
       " {462, 1048},\n",
       " {474, 1181},\n",
       " {538, 1286},\n",
       " {560, 585, 774, 2526},\n",
       " {587, 1032},\n",
       " {592, 2669},\n",
       " {611, 2690},\n",
       " {625, 1024},\n",
       " {635, 1378, 1544, 2058, 2150},\n",
       " {641, 2704},\n",
       " {653, 1231},\n",
       " {662, 932},\n",
       " {677, 954, 1112},\n",
       " {692, 2629},\n",
       " {700, 1691},\n",
       " {713, 1044},\n",
       " {721, 1034},\n",
       " {780, 2341},\n",
       " {786, 947},\n",
       " {832, 2600},\n",
       " {917, 2639},\n",
       " {939, 2173, 2174},\n",
       " {953, 2565, 2566, 2567},\n",
       " {959, 2529},\n",
       " {967, 2659},\n",
       " {974, 1496},\n",
       " {986, 2697},\n",
       " {1005, 1541},\n",
       " {1059, 1600},\n",
       " {1210, 1648},\n",
       " {1233, 2433},\n",
       " {1236, 2479},\n",
       " {1263, 1407},\n",
       " {1298, 2703},\n",
       " {1310, 2692},\n",
       " {1356, 1613},\n",
       " {1371, 1393},\n",
       " {1375, 2586},\n",
       " {1438, 2664},\n",
       " {1554, 1657, 2686, 2687, 2688},\n",
       " {1563, 2633},\n",
       " {1673, 2660},\n",
       " {2410, 2411},\n",
       " {2431, 2432},\n",
       " {2602, 2603},\n",
       " {2618, 2619},\n",
       " {2625, 2626},\n",
       " {2634, 2635, 2636, 2693},\n",
       " {2665, 2666}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(list(nx.connected_components(G.to_undirected())))) # 查看有多少联通分量\n",
    "list(nx.connected_components(G.to_undirected()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13e5067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9492256704888787\n",
      "0.947189233070174\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.8,stratify=y,random_state=123)\n",
    "\n",
    "clf = lgb.LGBMClassifier(max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovr'))\n",
    "print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff61641",
   "metadata": {},
   "source": [
    "## 1.3 使用图嵌入\n",
    "使用node2vec，引入节点的拓扑信息的表征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1408c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法1：\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b708b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #设置node2vec参数\n",
    "# node2vec_ = Node2Vec(G,dimensions=32, #嵌入向量维度\n",
    "#                      p=1, #回家参数\n",
    "#                      q=3, #外出参数\n",
    "#                      walk_length=10, #随机游走最大长度\n",
    "#                      num_walks=600, #每个节点作为起始点生成的随机游走路径数\n",
    "#                      workers=4 #并行线程数\n",
    "#                     )\n",
    "# # p=1, q=0.5, n_clusters=6。DFS深度优先搜索，挖掘同质社群\n",
    "# # p=1, q=2, n_clusters=3。BFS宽度优先搜索，挖掘节点的结构功能。#训练Node2vec,参考文档见 gensim.models.Word2Vec\n",
    "# model = node2vec_.fit(window=3,  #skip-gram窗口大小\n",
    "#                       min_count=1, #忽略出现次数低于次数的节点，设置阈值\n",
    "#                       batch_words=4 #每个线程处理的数据量\n",
    "#                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02d79f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n2v = model.wv.vectors\n",
    "# print(n2v.shape) \n",
    "# #查看Embedding\n",
    "# shape_ = model.wv.get_vector('1').shape\n",
    "# print(shape_)\n",
    "# #(32,)#查看某个节点的embedding\n",
    "# embedding_ = model.wv.get_vector('1')\n",
    "# print(embedding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e183603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_embedding = {}\n",
    "# for node in G.nodes:\n",
    "#     node_embedding[node] = model.wv.get_vector(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc696ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_features = X.node_id.map(node_embedding)\n",
    "# embedding_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59db7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_features = np.stack(embedding_features, axis=0)\n",
    "# embedding_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a87bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_features = pd.DataFrame(embedding_features, columns=['emb_'+str(i) for i in range(embedding_features.shape[1])])\n",
    "# embedding_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eaefecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in embedding_features.columns:\n",
    "#     X[col] = embedding_features[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a464fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat([X,embedding_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75a24eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5ef3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid = train_test_split(df,y,train_size=0.8,stratify=y,random_state=123)\n",
    "\n",
    "# clf = lgb.LGBMClassifier(max_depth=3)\n",
    "# clf.fit(X_train, y_train)\n",
    "# print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovr'))\n",
    "# print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0cee650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c82bff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\aconda\\envs\\pytorch\\lib\\site-packages\\networkx\\linalg\\graphmatrix.py:187: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  return adjacency_matrix(G, nodelist, dtype, weight)\n"
     ]
    }
   ],
   "source": [
    "from nodevectors import ProNE\n",
    "n2v_ = ProNE(n_components=32)\n",
    "n2v_.fit(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ba0c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embedding = {}\n",
    "for node in G.nodes:\n",
    "    node_embedding[node] = n2v_.predict(node)\n",
    "embedding_features = X.node_id.map(node_embedding)\n",
    "embedding_features = np.stack(embedding_features, axis=0)\n",
    "embedding_features.shape\n",
    "embedding_features = pd.DataFrame(embedding_features, columns=['emb_'+str(i) for i in range(embedding_features.shape[1])])\n",
    "\n",
    "df2 = pd.concat([X,embedding_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dcfc3354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_22</th>\n",
       "      <th>emb_23</th>\n",
       "      <th>emb_24</th>\n",
       "      <th>emb_25</th>\n",
       "      <th>emb_26</th>\n",
       "      <th>emb_27</th>\n",
       "      <th>emb_28</th>\n",
       "      <th>emb_29</th>\n",
       "      <th>emb_30</th>\n",
       "      <th>emb_31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025145</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>-0.005791</td>\n",
       "      <td>-0.022498</td>\n",
       "      <td>-0.030820</td>\n",
       "      <td>-0.014778</td>\n",
       "      <td>0.008814</td>\n",
       "      <td>0.032128</td>\n",
       "      <td>0.148016</td>\n",
       "      <td>0.587833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030883</td>\n",
       "      <td>-0.045450</td>\n",
       "      <td>0.206974</td>\n",
       "      <td>-0.125497</td>\n",
       "      <td>0.066530</td>\n",
       "      <td>-0.116691</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>0.030550</td>\n",
       "      <td>0.099273</td>\n",
       "      <td>0.070968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041783</td>\n",
       "      <td>-0.111247</td>\n",
       "      <td>0.020196</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>0.492620</td>\n",
       "      <td>0.071931</td>\n",
       "      <td>0.101089</td>\n",
       "      <td>0.263065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131912</td>\n",
       "      <td>0.094369</td>\n",
       "      <td>0.055185</td>\n",
       "      <td>-0.283367</td>\n",
       "      <td>0.221676</td>\n",
       "      <td>0.234822</td>\n",
       "      <td>0.199051</td>\n",
       "      <td>0.071967</td>\n",
       "      <td>-0.262039</td>\n",
       "      <td>-0.386990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141980</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>-0.310852</td>\n",
       "      <td>-0.134201</td>\n",
       "      <td>-0.212942</td>\n",
       "      <td>0.260197</td>\n",
       "      <td>0.109889</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>-0.042834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>2703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067067</td>\n",
       "      <td>0.441672</td>\n",
       "      <td>-0.214737</td>\n",
       "      <td>-0.132747</td>\n",
       "      <td>0.149483</td>\n",
       "      <td>0.071430</td>\n",
       "      <td>0.034131</td>\n",
       "      <td>-0.037980</td>\n",
       "      <td>0.065397</td>\n",
       "      <td>-0.009703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>2704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078381</td>\n",
       "      <td>-0.151276</td>\n",
       "      <td>0.260473</td>\n",
       "      <td>-0.498793</td>\n",
       "      <td>0.130182</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>-0.097518</td>\n",
       "      <td>0.032703</td>\n",
       "      <td>0.062985</td>\n",
       "      <td>-0.008407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>2705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012143</td>\n",
       "      <td>-0.250367</td>\n",
       "      <td>0.056457</td>\n",
       "      <td>-0.042739</td>\n",
       "      <td>-0.064134</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>-0.121135</td>\n",
       "      <td>-0.012935</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>-0.119308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>2706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035080</td>\n",
       "      <td>-0.023393</td>\n",
       "      <td>0.026374</td>\n",
       "      <td>0.121077</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.143851</td>\n",
       "      <td>0.163036</td>\n",
       "      <td>-0.017640</td>\n",
       "      <td>-0.011307</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>2707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043018</td>\n",
       "      <td>-0.021541</td>\n",
       "      <td>0.044410</td>\n",
       "      <td>0.092108</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.122933</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>-0.036094</td>\n",
       "      <td>-0.021532</td>\n",
       "      <td>0.019207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2708 rows × 1470 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      node_id    0    1    2    3    4    5    6    7    8  ...    emb_22  \\\n",
       "0           0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.025145   \n",
       "1           1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.030883   \n",
       "2           2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.041783   \n",
       "3           3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.131912   \n",
       "4           4  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ... -0.141980   \n",
       "...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "2703     2703  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ... -0.067067   \n",
       "2704     2704  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.078381   \n",
       "2705     2705  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.012143   \n",
       "2706     2706  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.035080   \n",
       "2707     2707  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.043018   \n",
       "\n",
       "        emb_23    emb_24    emb_25    emb_26    emb_27    emb_28    emb_29  \\\n",
       "0    -0.000809 -0.005791 -0.022498 -0.030820 -0.014778  0.008814  0.032128   \n",
       "1    -0.045450  0.206974 -0.125497  0.066530 -0.116691  0.163054  0.030550   \n",
       "2    -0.111247  0.020196  0.061493  0.019615  0.013267  0.492620  0.071931   \n",
       "3     0.094369  0.055185 -0.283367  0.221676  0.234822  0.199051  0.071967   \n",
       "4     0.120287  0.131507 -0.310852 -0.134201 -0.212942  0.260197  0.109889   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2703  0.441672 -0.214737 -0.132747  0.149483  0.071430  0.034131 -0.037980   \n",
       "2704 -0.151276  0.260473 -0.498793  0.130182  0.010768 -0.097518  0.032703   \n",
       "2705 -0.250367  0.056457 -0.042739 -0.064134  0.218479 -0.121135 -0.012935   \n",
       "2706 -0.023393  0.026374  0.121077  0.000434  0.143851  0.163036 -0.017640   \n",
       "2707 -0.021541  0.044410  0.092108  0.004525  0.122933  0.049860 -0.036094   \n",
       "\n",
       "        emb_30    emb_31  \n",
       "0     0.148016  0.587833  \n",
       "1     0.099273  0.070968  \n",
       "2     0.101089  0.263065  \n",
       "3    -0.262039 -0.386990  \n",
       "4    -0.016846 -0.042834  \n",
       "...        ...       ...  \n",
       "2703  0.065397 -0.009703  \n",
       "2704  0.062985 -0.008407  \n",
       "2705  0.033285 -0.119308  \n",
       "2706 -0.011307  0.026611  \n",
       "2707 -0.021532  0.019207  \n",
       "\n",
       "[2708 rows x 1470 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b26bce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816269764113018\n",
      "0.9821537997129604\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(df2,y,train_size=0.8,stratify=y,random_state=123)\n",
    "\n",
    "clf = lgb.LGBMClassifier(max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovr'))\n",
    "print(metrics.roc_auc_score(y_valid, clf.predict_proba(X_valid), average='macro', multi_class='ovo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d7e869",
   "metadata": {},
   "source": [
    "## 1.4 使用图深度学习算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c769d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\programs\\aconda\\envs\\pytorch\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "train_nodes, test_nodes = train_test_split(dataset.data.y, train_size=0.8, random_state=123, stratify=dataset.data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1f678cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "\n",
    "label_encoding = preprocessing.LabelBinarizer()\n",
    "train_labels = label_encoding.fit_transform(train_nodes)\n",
    "test_labels = label_encoding.fit_transform(test_nodes)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe661b",
   "metadata": {},
   "source": [
    "### 1.4.1分类基线模型，进行节点分类\n",
    "\n",
    "该模型忽略节点连接(或图结构)，并试图仅使用词向量对节点标签进行分类。模型类如下,它有两个隐藏层(Linear)，带有ReLU激活，后面是一个输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e2e48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 将节点分为train、valid和test(替换数据中的原始分割掩码，因为它的训练集太小了)\n",
    "split = T.RandomNodeSplit(num_val=0.1, num_test=0.2)\n",
    "data = split(dataset[0])     \n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(dataset[0].num_node_features, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, dataset.num_classes))\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        output = self.layers(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae879214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            pred = torch.max(outputs[data.train_mask].data, 1)[1].numpy()\n",
    "            train_acc = metrics.accuracy_score(data.y[data.train_mask].numpy(), pred) # 训练集精度\n",
    "            # 获取验证集上的精度\n",
    "            dev_acc = evaluate(model, data, data.val_mask, criterion)\n",
    "            print(f'Epoch: {epoch:d}, Train Loss: {loss:.3f}, Val Acc: {dev_acc:.3f}')\n",
    "    return model\n",
    "\n",
    "def evaluate(model, data, mask, criterion):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "#     pred = model(data).argmax(dim=1)\n",
    "#     correct = (pred[mask] == data.y[mask]).sum()\n",
    "#     acc = int(correct) / int(mask.sum())\n",
    "    y_all = np.array([], dtype=int)\n",
    "    pred_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs[mask], data.y[mask])\n",
    "        loss_total += loss\n",
    "        y = data.y[mask].to(device).numpy()\n",
    "        pred = torch.max(outputs[mask].data, 1)[1].to(device).numpy()\n",
    "        y_all = np.append(y_all, y)\n",
    "        pred_all = np.append(pred_all, pred)  \n",
    "    acc = metrics.accuracy_score(y_all, pred_all)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3ed7003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 1.941, Val Acc: 0.343\n",
      "Epoch: 10, Train Loss: 0.818, Val Acc: 0.690\n",
      "Epoch: 20, Train Loss: 0.075, Val Acc: 0.738\n",
      "Epoch: 30, Train Loss: 0.016, Val Acc: 0.734\n",
      "Epoch: 40, Train Loss: 0.014, Val Acc: 0.738\n",
      "Epoch: 50, Train Loss: 0.015, Val Acc: 0.712\n",
      "Epoch: 60, Train Loss: 0.013, Val Acc: 0.720\n",
      "Epoch: 70, Train Loss: 0.010, Val Acc: 0.705\n",
      "Epoch: 80, Train Loss: 0.009, Val Acc: 0.686\n",
      "Epoch: 90, Train Loss: 0.008, Val Acc: 0.690\n",
      "Epoch: 100, Train Loss: 0.008, Val Acc: 0.694\n",
      "Epoch: 110, Train Loss: 0.008, Val Acc: 0.705\n",
      "Epoch: 120, Train Loss: 0.007, Val Acc: 0.708\n",
      "Epoch: 130, Train Loss: 0.007, Val Acc: 0.712\n",
      "Epoch: 140, Train Loss: 0.007, Val Acc: 0.716\n",
      "Test Acc: 0.721\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP().to(device)\n",
    "optimizer_mlp = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp = train(mlp, data, optimizer_mlp, criterion, n_epochs=150)\n",
    "test_acc = evaluate(mlp, data, data.test_mask, criterion)\n",
    "print(f'Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9beadd",
   "metadata": {},
   "source": [
    "### 1.4.2 使用GCN进行节点分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90b619c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 10, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 20, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 30, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 40, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 50, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 60, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 70, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 80, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 90, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 100, Train Loss: 1.935, Val Acc: 0.240\n",
      "Epoch: 110, Train Loss: 1.935, Val Acc: 0.240\n",
      "Test Acc: 0.210\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16\n",
    "                             , dataset.num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        output = self.conv2(x, edge_index)\n",
    "        return output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gcn = GCN().to(device)\n",
    "optimizer_gcn = torch.optim.Adam(gcn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "gcn = train(gcn, data, optimizer_mlp, criterion, n_epochs=120)\n",
    "test_acc = evaluate(gcn, data, data.test_mask, criterion)\n",
    "print(f'Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25eae74",
   "metadata": {},
   "source": [
    "## 任务二：链接预测\n",
    "1. 编码器使用两个卷积层的图来创建节点嵌入。\n",
    "2. 在原始图上随机添加负链接，模型任务变为对原始的正链接和新增的负链接进行二分类。\n",
    "3. 解码器使用节点嵌入对所有边（包含负链接）进行链接预测（二分类）。从每条边上的一对节点计算节点嵌入的点积，然后聚合整个嵌入维度的值，并在每条边上创建一个表示边存在概率的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf51d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        \n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "    \n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]]*z[edge_label_index[1]]).sum(dim=-1)\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj>0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd8a944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link_predictor(model, train_data, val_data, optimizer, criterion, n_epochs):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "        \n",
    "        \n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "            num_neg_samples=train_data.edge_label_index.size(1), method='sparse'\n",
    "        )\n",
    "        \n",
    "        edge_label_index = torch.cat([train_data.edge_label_index, neg_edge_index], dim=-1)\n",
    "        \n",
    "        edge_label = torch.cat([train_data.edge_label, train_data.edge_label.new_zeros(neg_edge_index.size(1))],dim=0)\n",
    "        \n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        val_auc = eval_link_predictor(model, val_data)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch:d}, Train Loss: {loss:.3f}, Val AUC: {val_auc:.3f}\")\n",
    "    return model\n",
    "        \n",
    "@torch.no_grad()\n",
    "def eval_link_predictor(model, data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8c77581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "# 使用PyG中的RandomNodeSplit模块将节点分为train、valid和test(我替换数据中的原始分割掩码，因为它的训练集太小了\n",
    "split = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=False,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "data = dataset[0]\n",
    "train_data, val_data, test_data = split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7107b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "train_data: Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[4224], edge_label_index=[2, 4224])\n",
      "val_data: Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n",
      "test_data: Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n"
     ]
    }
   ],
   "source": [
    "print(\"data:\", data)\n",
    "print(\"train_data:\", train_data)\n",
    "print(\"val_data:\", val_data)\n",
    "print(\"test_data:\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5a496df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.665, Val AUC: 0.797\n",
      "Epoch: 20, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 30, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 40, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 50, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 60, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 70, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 80, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 90, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 100, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 110, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 120, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 130, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 140, Train Loss: 0.664, Val AUC: 0.797\n",
      "Epoch: 150, Train Loss: 0.664, Val AUC: 0.797\n",
      "Test: 0.807\n"
     ]
    }
   ],
   "source": [
    "model = Net(dataset.num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "n_epochs = 150\n",
    "model = train_link_predictor(model, train_data, val_data, optimizer, criterion, n_epochs=150)\n",
    " \n",
    "test_auc = eval_link_predictor(model, test_data)\n",
    "print(f\"Test: {test_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e8d1e",
   "metadata": {},
   "source": [
    "## 任务三：异常检测  \n",
    "数据集有两种不同类型的异常值:  \n",
    "* 1.结构异常  \n",
    "    密集连接的节点，而不是稀疏连接的规则节点  \n",
    "* 2.上下文的异常值  \n",
    "    属性与相邻节点显著不同的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff81e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0:正常，1:仅上下文异常，2:结构异常，3:上下文和结构都异常\n",
    "# PyGOD库,是建立在PyG之上的一个图异常值检测库。可以通过PyGOD模块加载已经进行了异常值注入的Cora数据集。\n",
    "from pygod.utils import load_data\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "032665b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 11060], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data('inj_cora','./dataset')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2dcab79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2570, 1: 68, 2: 68, 3: 2})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(data.y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5c8debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    " \n",
    "split = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.2,\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=False,\n",
    "    neg_sampling_ratio=1.0,\n",
    ")\n",
    "train_data, val_data, test_data = split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "281dd1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 7750], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[3875], edge_label_index=[2, 3875])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d49e7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygod.detector import DOMINANT\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def train_anormal_detector(model, data):\n",
    "    return model.fit(data)\n",
    "\n",
    "def eval_anomal_detector(model, data):\n",
    "    score = model.decision_score_\n",
    "    pred, score = model.predict(data, return_score=True)\n",
    "    correct = (pred == data.y).sum()\n",
    "    acc = int(correct)/int(data.y.shape[0])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DOMINANT()\n",
    "model = train_anormal_detector(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e650270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_anomal_detector(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f82d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349bd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
